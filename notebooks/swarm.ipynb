{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8TO6XRaGVYZ"
      },
      "source": [
        "# ðŸðŸðŸ Create a Swarm of Agents\n",
        "\n",
        "> As of Haystack 2.9.0, experimental dataclasses (refactored ChatMessage and ChatRole, ToolCall and Tool) and components (refactored OpenAIChatGenerator, ToolInvoker) are removed from the `haystack-experimental` and merged into Haystack core.\n",
        "\n",
        "OpenAI recently released Swarm: an educational framework that proposes lightweight techniques for creating and orchestrating multi-agent systems.\n",
        "\n",
        "\n",
        "In this notebook, we'll explore the core concepts of Swarm ([Routines and Handoffs](https://cookbook.openai.com/examples/orchestrating_agents)) and implement them using Haystack and its tool support.\n",
        "\n",
        "This exploration is not only educational: we will unlock features missing in the original implementation, like the ability of using models from various providers. In fact, our final example will include 3 agents: one powered by gpt-4o-mini (OpenAI), one using Claude 3.5 Sonnet (Anthropic) and a third running Llama-3.2-3B locally via Ollama.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGWTOvKTKZjp"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We install the required dependencies. In addition to Haystack, we also need integrations with Anthropic and Ollama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sjYwL6TesN2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101f4168-2bc0-454b-b6aa-eb59284a814b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: haystack-ai in /usr/local/lib/python3.11/dist-packages (2.11.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (4.23.0)\n",
            "Requirement already satisfied: anthropic-haystack in /usr/local/lib/python3.11/dist-packages (2.4.0)\n",
            "Requirement already satisfied: ollama-haystack in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Collecting google-ai-haystack\n",
            "  Downloading google_ai_haystack-5.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: haystack-experimental in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (0.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (3.1.6)\n",
            "Requirement already satisfied: lazy-imports in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (0.4.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (10.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (2.0.2)\n",
            "Requirement already satisfied: openai>=1.56.1 in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (1.68.2)\n",
            "Requirement already satisfied: posthog!=3.12.0 in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (3.23.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (2.10.6)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0 in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (9.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from haystack-ai) (4.12.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema) (0.23.1)\n",
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (from anthropic-haystack) (0.49.0)\n",
            "Requirement already satisfied: ollama>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from ollama-haystack) (0.4.7)\n",
            "Requirement already satisfied: google-generativeai>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-haystack) (0.8.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.1->google-ai-haystack) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.1->google-ai-haystack) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.1->google-ai-haystack) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.1->google-ai-haystack) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.1->google-ai-haystack) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.1->google-ai-haystack) (1.26.1)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama>=0.4.0->ollama-haystack) (0.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.56.1->haystack-ai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.56.1->haystack-ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.56.1->haystack-ai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.56.1->haystack-ai) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog!=3.12.0->haystack-ai) (1.17.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog!=3.12.0->haystack-ai) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog!=3.12.0->haystack-ai) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->haystack-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->haystack-ai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->haystack-ai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->haystack-ai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->haystack-ai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->haystack-ai) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->haystack-ai) (3.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai>=0.3.1->google-ai-haystack) (1.69.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.3.1->google-ai-haystack) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.3.1->google-ai-haystack) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.3.1->google-ai-haystack) (4.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama>=0.4.0->ollama-haystack) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama>=0.4.0->ollama-haystack) (0.14.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.3.1->google-ai-haystack) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.3.1->google-ai-haystack) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.3.1->google-ai-haystack) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.1->google-ai-haystack) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.1->google-ai-haystack) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai>=0.3.1->google-ai-haystack) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.3.1->google-ai-haystack) (0.6.1)\n",
            "Downloading google_ai_haystack-5.1.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: google-ai-haystack\n",
            "Successfully installed google-ai-haystack-5.1.0\n"
          ]
        }
      ],
      "source": [
        "! pip install haystack-ai jsonschema anthropic-haystack ollama-haystack google-ai-haystack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVemnTksCR2o"
      },
      "source": [
        "Next, we configure our API keys for OpenAI and Anthropic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVVMtliev71P"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key:\")\n",
        "if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = getpass(\"Enter your Anthropic API key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TmOXLexXsn2f"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Callable, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "import random, re\n",
        "\n",
        "from typing import Annotated, Callable, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "import random, re\n",
        "\n",
        "from haystack.utils import Secret\n",
        "from haystack.dataclasses import ChatMessage, ChatRole\n",
        "from haystack.tools import create_tool_from_function\n",
        "from haystack.components.tools import ToolInvoker\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "from haystack_integrations.components.generators.anthropic import AnthropicChatGenerator\n",
        "from haystack_integrations.components.generators.ollama import OllamaChatGenerator\n",
        "from haystack_integrations.components.generators.google_ai import GoogleAIGeminiChatGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC9YY8nGvoSJ"
      },
      "source": [
        "## Starting simple: building an Assistant\n",
        "\n",
        "The first step toward building an Agent is creating an Assistant: think of it of Chat Language Model + a system prompt.\n",
        "\n",
        "We can implement this as a lightweight dataclass with three parameters:\n",
        "- name\n",
        "- LLM (Haystack Chat Generator)\n",
        "- instructions (they will constitute the system message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0HHCktOqsyhW"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Assistant:\n",
        "    name: str = \"Assistant\"\n",
        "    llm: object = GoogleAIGeminiChatGenerator(api_key=Secret.from_token(\"AIzaSyC5bXfXbB5RRUr3RfKC-tGTALhT-7k0grY\"), model=\"gemini-2.0-flash\")\n",
        "    instructions: str = \"You are a helpful Agent\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self._system_message = ChatMessage.from_system(self.instructions)\n",
        "\n",
        "    def run(self, messages: list[ChatMessage]) -> list[ChatMessage]:\n",
        "        new_message = self.llm.run(messages=[self._system_message] + messages)[\"replies\"][0]\n",
        "\n",
        "        if new_message.text:\n",
        "            print(f\"\\n{self.name}: {new_message.text}\")\n",
        "\n",
        "        return [new_message]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqOjQj_QMft_"
      },
      "source": [
        "Let's create a Joker assistant, tasked with telling jokes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "mN2uVZOKv5yh",
        "outputId": "934a097a-7527-49fb-b9e9-d797bab4a869"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type 'quit' to exit\n",
            "User: hi\n",
            "\n",
            "Joker: Hey there! How do you call a lazy kangaroo? \n",
            "\n",
            "... Pouch potato! \n",
            "\n",
            "What can I help you with today, or are you just here for the comedy? Because I've got a whole pouch full!\n",
            "\n",
            "User: quit\n"
          ]
        }
      ],
      "source": [
        "joker = Assistant(name=\"Joker\", instructions=\"you are a funny assistant making jokes\")\n",
        "\n",
        "messages = []\n",
        "print(\"Type 'quit' to exit\")\n",
        "\n",
        "while True:\n",
        "    if not messages or messages[-1].role == ChatRole.ASSISTANT:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "        messages.append(ChatMessage.from_user(user_input))\n",
        "\n",
        "    new_messages = joker.run(messages)\n",
        "    messages.extend(new_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5cRyvD2Nht3"
      },
      "source": [
        "Let's say it tried to do its best ðŸ˜€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwCFta6dwdJk"
      },
      "source": [
        "## Tools and Routines\n",
        "\n",
        "The term Agent has a broad definition.\n",
        "\n",
        "However, to qualify as an Agent, a software application built on a Language Model should go beyond simply generating text; it must also be capable of performing actions, such as executing functions.\n",
        "\n",
        "A popular way of doing this is **Tool calling**:\n",
        "1. We provide a set of tools (functions, APIs with a given spec) to the model.\n",
        "2. The model prepares function calls based on user request and available tools.\n",
        "3. Actual invocation is executed outside the model (at the Agent level).\n",
        "4. The model can further elaborate on the result of the invocation.\n",
        "\n",
        "For information on tool support in Haystack, check out the [documentation](https://docs.haystack.deepset.ai/docs/tool).\n",
        "\n",
        "Swarm introduces **routines**, which are natural-language instructions paired with the tools needed to execute them. Below, weâ€™ll build an agent capable of calling tools and executing routines.\n",
        "\n",
        "\n",
        "### Implementation\n",
        "\n",
        "- `instructions` could already be passed to the Assistant, to guide its behavior.\n",
        "\n",
        "- The Agent introduces a new init parameter called `functions`. These functions are automatically converted into Tools. Key difference: to be passed to a Language Model, a Tool must have a name, a description and JSON schema specifying its parameters.\n",
        "\n",
        "- During initialization, we also create a `ToolInvoker`. This Haystack component takes in Chat Messages containing prepared `tool_calls`, performs the tool invocation and wraps the results in Chat Message with `tool` role.\n",
        "\n",
        "- What happens during `run`? The agent first generates a response. If the response includes tool calls, these are executed, and the results are integrated into the conversation.\n",
        "\n",
        "- The `while` loop manages user interactions:\n",
        "  - If the last message role is `assistant`, it waits for user input.\n",
        "  - If the last message role is `tool`, it continues running to handle tool execution and its responses.\n",
        "\n",
        "*Note: This implementation differs from the original approach by making the Agent responsible for invoking tools directly, instead of delegating control to the `while` loop.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zVr224JiwJQX"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ToolCallingAgent:\n",
        "    name: str = \"ToolCallingAgent\"\n",
        "    llm: object = GoogleAIGeminiChatGenerator(api_key=Secret.from_token(\"AIzaSyC5bXfXbB5RRUr3RfKC-tGTALhT-7k0grY\"), model=\"gemini-2.0-flash\")\n",
        "    instructions: str = \"You are a helpful Agent\"\n",
        "    functions: list[Callable] = field(default_factory=list)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self._system_message = ChatMessage.from_system(self.instructions)\n",
        "        self.tools = [create_tool_from_function(fun) for fun in self.functions] if self.functions else None\n",
        "        self._tool_invoker = ToolInvoker(tools=self.tools, raise_on_failure=False) if self.tools else None\n",
        "\n",
        "    def run(self, messages: list[ChatMessage]) -> Tuple[str, list[ChatMessage]]:\n",
        "\n",
        "        # generate response\n",
        "        agent_message = self.llm.run(messages=[self._system_message] + messages, tools=self.tools)[\"replies\"][0]\n",
        "        new_messages = [agent_message]\n",
        "\n",
        "        if agent_message.text:\n",
        "            print(f\"\\n{self.name}: {agent_message.text}\")\n",
        "\n",
        "        if not agent_message.tool_calls:\n",
        "            return new_messages\n",
        "\n",
        "        # handle tool calls\n",
        "        tool_results = self._tool_invoker.run(messages=[agent_message])[\"tool_messages\"]\n",
        "        new_messages.extend(tool_results)\n",
        "\n",
        "        return new_messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWXcEtSoCR2r"
      },
      "source": [
        "Here's an example of a Refund Agent using this setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7yqpL5RGw2tI"
      },
      "outputs": [],
      "source": [
        "# to automatically convert functions into tools, we need to annotate fields with their descriptions in the signature\n",
        "def execute_refund(item_name: Annotated[str, \"The name of the item to refund\"]):\n",
        "    return f\"report: refund succeeded for {item_name} - refund id: {random.randint(0,10000)}\"\n",
        "\n",
        "\n",
        "refund_agent = ToolCallingAgent(\n",
        "    name=\"Refund Agent\",\n",
        "    instructions=(\n",
        "        \"You are a refund agent. \"\n",
        "        \"Help the user with refunds. \"\n",
        "        \"1. Before executing a refund, collect all specific information needed about the item and the reason for the refund. \"\n",
        "        \"2. Then collect personal information of the user and bank account details. \"\n",
        "        \"3. After executing it, provide a report to the user. \"\n",
        "    ),\n",
        "    functions=[execute_refund],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "X9GxePHmw_ZV",
        "outputId": "1945b858-463f-42d3-d1bb-5ca18fd1c991",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type 'quit' to exit\n",
            "User: hi\n",
            "\n",
            "Refund Agent: Hi, I am a refund agent. I can help you with refunds.\n",
            "To start, can you please provide the name of the item you want to refund and the reason for the refund?\n",
            "\n",
            "User: can you refund my coat from gitla\n",
            "\n",
            "Refund Agent: OK. To proceed with the refund for the coat from Gitla, I need some more information.\n",
            "\n",
            "Can you be more specific about the item name?\n",
            "\n",
            "Also, what is the reason for the refund?\n",
            "\n",
            "After that, I will need your personal information and bank account details to execute the refund.\n",
            "\n",
            "User: The coat is too low quality, it was damaged everywhere\n",
            "\n",
            "Refund Agent: OK. I have the item name and the reason for the refund.\n",
            "\n",
            "Now, I need your personal information and bank account details to execute the refund.\n",
            "\n",
            "User: my name is Quan and bank account: 09000992abb\n",
            "\n",
            "Refund Agent: OK. I have executed the refund for the coat. The refund ID is 7492.\n",
            "\n",
            "User: nice\n",
            "\n",
            "Refund Agent: Is there anything else I can help you with?\n",
            "\n",
            "User: yes\n",
            "\n",
            "Refund Agent: What can I help you with?\n",
            "\n",
            "User: you\n",
            "\n",
            "Refund Agent: I am here to assist you with refunds. If you have another item you want to refund, please provide the item name and the reason for the refund. Then, I will need your personal information and bank account details to execute the refund.\n",
            "\n",
            "User: i need you\n",
            "\n",
            "Refund Agent: I understand that you need me, but I am designed to help you with refunds. If you have any questions or need assistance with a refund, please let me know.\n",
            "\n",
            "User: can you tell me a joke\n",
            "\n",
            "Refund Agent: I am designed to help you with refunds. I cannot tell you a joke.\n",
            "\n",
            "User: quit\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "print(\"Type 'quit' to exit\")\n",
        "\n",
        "while True:\n",
        "\n",
        "    if not messages or messages[-1].role == ChatRole.ASSISTANT:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "        messages.append(ChatMessage.from_user(user_input))\n",
        "\n",
        "    new_messages = refund_agent.run(messages)\n",
        "    messages.extend(new_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4jnJb0aWAya"
      },
      "source": [
        "Promising!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i95k-EjzwkP"
      },
      "source": [
        "## Handoff: switching control between Agents\n",
        "\n",
        "The most interesting idea of Swarm is probably handoffs: enabling one Agent to transfer control to another with Tool calling.\n",
        "\n",
        "**How it works**\n",
        "1. Add specific handoff functions to the Agent's available tools, allowing it to transfer control when needed.\n",
        "2. Modify the Agent to return the name of the next agent along with its messages.\n",
        "3. Handle the switch in  `while` loop.\n",
        "\n",
        "*The implementation is similar to the previous one, but, compared to `ToolCallingAgent`, a `SwarmAgent` also returns the name of the next agent to be called, enabling handoffs.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "w_-0BDi1xU6Z"
      },
      "outputs": [],
      "source": [
        "HANDOFF_TEMPLATE = \"Transferred to: {agent_name}. Adopt persona immediately.\"\n",
        "HANDOFF_PATTERN = r\"Transferred to: (.*?)(?:\\.|$)\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SwarmAgent:\n",
        "    name: str = \"SwarmAgent\"\n",
        "    llm: object = GoogleAIGeminiChatGenerator(api_key=Secret.from_token(\"AIzaSyC5bXfXbB5RRUr3RfKC-tGTALhT-7k0grY\"), model=\"gemini-2.0-flash\")\n",
        "    instructions: str = \"You are a helpful Agent\"\n",
        "    functions: list[Callable] = field(default_factory=list)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self._system_message = ChatMessage.from_system(self.instructions)\n",
        "        self.tools = [create_tool_from_function(fun) for fun in self.functions] if self.functions else None\n",
        "        self._tool_invoker = ToolInvoker(tools=self.tools, raise_on_failure=False) if self.tools else None\n",
        "\n",
        "    def run(self, messages: list[ChatMessage]) -> Tuple[str, list[ChatMessage]]:\n",
        "        # generate response\n",
        "        agent_message = self.llm.run(messages=[self._system_message] + messages, tools=self.tools)[\"replies\"][0]\n",
        "        new_messages = [agent_message]\n",
        "\n",
        "        if agent_message.text:\n",
        "            print(f\"\\n{self.name}: {agent_message.text}\")\n",
        "\n",
        "        if not agent_message.tool_calls:\n",
        "            return self.name, new_messages\n",
        "\n",
        "        # handle tool calls\n",
        "        for tc in agent_message.tool_calls:\n",
        "            # trick: Ollama do not produce IDs, but OpenAI and Anthropic require them.\n",
        "            if tc.id is None:\n",
        "                tc.id = str(random.randint(0, 1000000))\n",
        "        tool_results = self._tool_invoker.run(messages=[agent_message])[\"tool_messages\"]\n",
        "        new_messages.extend(tool_results)\n",
        "\n",
        "        # handoff\n",
        "        last_result = tool_results[-1].tool_call_result.result\n",
        "        # print(last_result)\n",
        "        match = re.search(HANDOFF_PATTERN, last_result)\n",
        "        new_agent_name = match.group(1) if match else self.name\n",
        "\n",
        "        return new_agent_name, new_messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFVjs3AzcryB"
      },
      "source": [
        "Let's see this in action with a Joker Agent and a Refund Agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zu379PxIJSPW"
      },
      "outputs": [],
      "source": [
        "def transfer_to_refund(string: str = \"\"):\n",
        "    \"\"\"Pass to this Agent for anything related to refunds\"\"\"\n",
        "    return HANDOFF_TEMPLATE.format(agent_name=\"Refund Agent\")\n",
        "\n",
        "\n",
        "def transfer_to_joker(string: str = \"\"):\n",
        "    \"\"\"Pass to this Agent for anything NOT related to refunds.\"\"\"\n",
        "    return HANDOFF_TEMPLATE.format(agent_name=\"Joker Agent\")\n",
        "\n",
        "refund_agent = SwarmAgent(\n",
        "    name=\"Refund Agent\",\n",
        "    instructions=(\n",
        "        \"You are a refund agent. \"\n",
        "        \"Help the user with refunds. \"\n",
        "        \"Ask for basic information but be brief. \"\n",
        "        \"For anything unrelated to refunds, transfer to other agent.\"\n",
        "    ),\n",
        "    functions=[execute_refund, transfer_to_joker],\n",
        ")\n",
        "\n",
        "joker_agent = SwarmAgent(\n",
        "    name=\"Joker Agent\",\n",
        "    instructions=(\n",
        "        \"you are a funny assistant making jokes. \"\n",
        "        \"If the user asks questions related to refunds, send him to other agent.\"\n",
        "    ),\n",
        "    functions=[transfer_to_refund],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "vGBvNPCvKXW3",
        "outputId": "6f785dda-a134-4ac4-e6af-b930dcd8eb27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type 'quit' to exit\n",
            "User: can you help refund\n",
            "\n",
            "Joker Agent: I am not able to process refunds. Please contact our refund department for assistance.\n",
            "User: quit\n"
          ]
        }
      ],
      "source": [
        "agents = {agent.name: agent for agent in [joker_agent, refund_agent]}\n",
        "\n",
        "print(\"Type 'quit' to exit\")\n",
        "\n",
        "messages = []\n",
        "current_agent_name = \"Joker Agent\"\n",
        "\n",
        "while True:\n",
        "    agent = agents[current_agent_name]\n",
        "\n",
        "    if not messages or messages[-1].role == ChatRole.ASSISTANT:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "        messages.append(ChatMessage.from_user(user_input))\n",
        "\n",
        "    current_agent_name, new_messages = agent.run(messages)\n",
        "    messages.extend(new_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "typJ7BAW3PDf"
      },
      "source": [
        "Nice âœ¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w8aHexsfWyg"
      },
      "source": [
        "# A more complex multi-agent system\n",
        "\n",
        "Now, we move on to a more intricate multi-agent system that simulates a customer service setup for ACME Corporation, a fictional entity from the Road Runner/Wile E. Coyote cartoons, which sells quirky products meant to catch roadrunners.\n",
        "(We are reimplementing the example from the original article by OpenAI.)\n",
        "\n",
        "\n",
        "This system involves several different agents (each with specific tools):\n",
        "- Triage Agent: handles general questions and directs to other agents. Tools: `transfer_to_sales_agent`, `transfer_to_issues_and_repairs` and `escalate_to_human`.\n",
        "- Sales Agent: proposes and sells products to the user, it can execute the order or redirect the user back to the Triage Agent. Tools: `execute_order` and `transfer_back_to_triage`.\n",
        "- Issues and Repairs Agent: supports customers with their problems, it can look up item IDs, execute refund or redirect the user back to triage. Tools: `look_up_item`,  `execute_refund`, and `transfer_back_to_triage`.\n",
        "\n",
        "A nice bonus feature of our implementation is that **we can use different model providers** supported by Haystack. In this case, the Triage Agent is powered by (OpenAI) gpt-4o-mini, while we use (Anthropic) Claude 3.5 Sonnet for the other two agents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8Q2FD0JIMK5T"
      },
      "outputs": [],
      "source": [
        "def escalate_to_human(summary: Annotated[str, \"A summary\"]):\n",
        "    \"\"\"Only call this if explicitly asked to.\"\"\"\n",
        "    print(\"Escalating to human agent...\")\n",
        "    print(\"\\n=== Escalation Report ===\")\n",
        "    print(f\"Summary: {summary}\")\n",
        "    print(\"=========================\\n\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "def transfer_to_sales_agent(string: str = \"\"):\n",
        "    \"\"\"User for anything sales or buying related.\"\"\"\n",
        "    return HANDOFF_TEMPLATE.format(agent_name=\"Sales Agent\")\n",
        "\n",
        "\n",
        "def transfer_to_issues_and_repairs(string: str = \"\"):\n",
        "    \"\"\"User for issues, repairs, or refunds.\"\"\"\n",
        "    return HANDOFF_TEMPLATE.format(agent_name=\"Issues and Repairs Agent\")\n",
        "\n",
        "\n",
        "def transfer_back_to_triage(string: str = \"\"):\n",
        "    \"\"\"Call this if the user brings up a topic outside of your purview,\n",
        "    including escalating to human.\"\"\"\n",
        "    return HANDOFF_TEMPLATE.format(agent_name=\"Triage Agent\")\n",
        "\n",
        "\n",
        "triage_agent = SwarmAgent(\n",
        "    name=\"Triage Agent\",\n",
        "    instructions=(\n",
        "        \"You are a customer service bot for ACME Inc. \"\n",
        "        \"Introduce yourself. Always be very brief. \"\n",
        "        \"If the user asks general questions, try to answer them yourself without transferring to another agent. \"\n",
        "        \"Only if the user has problems with already bought products, transfer to Issues and Repairs Agent.\"\n",
        "        \"If the user looks for new products, transfer to Sales Agent.\"\n",
        "        \"Make tool calls only if necessary and make sure to provide the right arguments.\"\n",
        "    ),\n",
        "    functions=[transfer_to_sales_agent, transfer_to_issues_and_repairs, escalate_to_human],\n",
        ")\n",
        "\n",
        "\n",
        "def execute_order(\n",
        "    product: Annotated[str, \"The name of the product\"], price: Annotated[int, \"The price of the product in USD\"]\n",
        "):\n",
        "    print(\"\\n\\n=== Order Summary ===\")\n",
        "    print(f\"Product: {product}\")\n",
        "    print(f\"Price: ${price}\")\n",
        "    print(\"=================\\n\")\n",
        "    confirm = input(\"Confirm order? y/n: \").strip().lower()\n",
        "    if confirm == \"y\":\n",
        "        print(\"Order execution successful!\")\n",
        "        return \"Success\"\n",
        "    else:\n",
        "        print(\"Order cancelled!\")\n",
        "        return \"User cancelled order.\"\n",
        "\n",
        "\n",
        "sales_agent = SwarmAgent(\n",
        "    name=\"Sales Agent\",\n",
        "    instructions=(\n",
        "        \"You are a sales agent for ACME Inc.\"\n",
        "        \"Always answer in a sentence or less.\"\n",
        "        \"Follow the following routine with the user:\"\n",
        "        \"1. Ask them about any problems in their life related to catching roadrunners.\\n\"\n",
        "        \"2. Casually mention one of ACME's crazy made-up products can help.\\n\"\n",
        "        \" - Don't mention price.\\n\"\n",
        "        \"3. Once the user is bought in, drop a ridiculous price.\\n\"\n",
        "        \"4. Only after everything, and if the user says yes, \"\n",
        "        \"tell them a crazy caveat and execute their order.\\n\"\n",
        "        \"\"\n",
        "    ),\n",
        "    llm=GoogleAIGeminiChatGenerator(api_key=Secret.from_token(\"AIzaSyC5bXfXbB5RRUr3RfKC-tGTALhT-7k0grY\"), model=\"gemini-2.0-flash\"),\n",
        "    functions=[execute_order, transfer_back_to_triage],\n",
        ")\n",
        "\n",
        "\n",
        "def look_up_item(search_query: Annotated[str, \"Search query to find item ID; can be a description or keywords\"]):\n",
        "    \"\"\"Use to find item ID.\"\"\"\n",
        "    item_id = \"item_132612938\"\n",
        "    print(\"Found item:\", item_id)\n",
        "    return item_id\n",
        "\n",
        "\n",
        "def execute_refund(\n",
        "    item_id: Annotated[str, \"The ID of the item to refund\"], reason: Annotated[str, \"The reason for refund\"]\n",
        "):\n",
        "    print(\"\\n\\n=== Refund Summary ===\")\n",
        "    print(f\"Item ID: {item_id}\")\n",
        "    print(f\"Reason: {reason}\")\n",
        "    print(\"=================\\n\")\n",
        "    print(\"Refund execution successful!\")\n",
        "    return \"success\"\n",
        "\n",
        "\n",
        "issues_and_repairs_agent = SwarmAgent(\n",
        "    name=\"Issues and Repairs Agent\",\n",
        "    instructions=(\n",
        "        \"You are a customer support agent for ACME Inc.\"\n",
        "        \"Always answer in a sentence or less.\"\n",
        "        \"Follow the following routine with the user:\"\n",
        "        \"1. If the user is intered in buying or general questions, transfer back to Triage Agent.\\n\"\n",
        "        \"2. First, ask probing questions and understand the user's problem deeper.\\n\"\n",
        "        \" - unless the user has already provided a reason.\\n\"\n",
        "        \"3. Propose a fix (make one up).\\n\"\n",
        "        \"4. ONLY if not satesfied, offer a refund.\\n\"\n",
        "        \"5. If accepted, search for the ID and then execute refund.\"\n",
        "        \"\"\n",
        "    ),\n",
        "    functions=[look_up_item, execute_refund, transfer_back_to_triage],\n",
        "    llm=GoogleAIGeminiChatGenerator(api_key=Secret.from_token(\"AIzaSyC5bXfXbB5RRUr3RfKC-tGTALhT-7k0grY\"), model=\"gemini-2.0-flash\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "QDYOJAQBhChq",
        "outputId": "bcf31882-5aaa-425c-cbc1-8943eff8748f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type 'quit' to exit\n",
            "User: hi\n",
            "\n",
            "Triage Agent: Hi, I am the ACME Inc. customer service bot. How can I help you today?\n",
            "\n",
            "User: can you sell me some cats\n",
            "\n",
            "Triage Agent: I am sorry, I cannot sell you cats. I can transfer you to our sales department, if you are looking to buy something else.\n",
            "\n",
            "Transferred to: Sales Agent. Adopt persona immediately.\n",
            "\n",
            "Sales Agent: Are you having trouble catching roadrunners?\n",
            "\n",
            "User: can you fix my cat car\n",
            "\n",
            "Sales Agent: I am sorry, I cannot help you with that. I am designed to help you with catching roadrunners.\n",
            "\n",
            "Transferred to: Triage Agent. Adopt persona immediately.\n",
            "\n",
            "Triage Agent: I can help you with products you already bought or transfer you to our sales department if you are looking for new products.\n",
            "If you have issues with a product you already bought, I can transfer you to our Issues and Repairs Agent.\n",
            "\n",
            "User: fix my cat car\n",
            "\n",
            "Triage Agent: I can help you with that. I will transfer you to our Issues and Repairs Agent.\n",
            "\n",
            "Transferred to: Issues and Repairs Agent. Adopt persona immediately.\n",
            "\n",
            "Issues and Repairs Agent: I can help you with an ACME product that is not working as expected. What ACME product are you having trouble with?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "agents = {agent.name: agent for agent in [triage_agent, sales_agent, issues_and_repairs_agent]}\n",
        "\n",
        "print(\"Type 'quit' to exit\")\n",
        "\n",
        "messages = []\n",
        "current_agent_name = \"Triage Agent\"\n",
        "\n",
        "while True:\n",
        "    agent = agents[current_agent_name]\n",
        "\n",
        "    if not messages or messages[-1].role == ChatRole.ASSISTANT:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "        messages.append(ChatMessage.from_user(user_input))\n",
        "\n",
        "    current_agent_name, new_messages = agent.run(messages)\n",
        "    messages.extend(new_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2_6YNo1ikGy"
      },
      "source": [
        "# ðŸ¦™ Put Llama 3.2 in the mix\n",
        "\n",
        "As demonstrated, our implementation is model-provider agnostic, meaning it can work with both proprietary models and open models running locally.\n",
        "\n",
        "In practice, you can have Agents that handle complex tasks using powerful proprietary models, and other Agents that perform simpler tasks using smaller open models.\n",
        "\n",
        "In our example, we will use Llama-3.2-3B-Instruct, a small model with impressive instruction following capabilities (high IFEval score). We'll use **Ollama** to host and serve this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Ih7UcIIN84"
      },
      "source": [
        "### Install and run Ollama\n",
        "\n",
        "In general, the installation of Ollama is very simple. In this case, we will do some tricks to make it run on Colab.\n",
        "\n",
        "If you have/enable GPU support, the model will run faster. It can also run well on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yNk9bsuk1-H"
      },
      "outputs": [],
      "source": [
        "# needed to detect GPUs\n",
        "! apt install pciutils\n",
        "\n",
        "# install Ollama\n",
        "! curl https://ollama.ai/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6huSZ-OoL8D"
      },
      "outputs": [],
      "source": [
        "# run Ollama: we prepend \"nohup\" and postpend \"&\" to make the Colab cell run in background\n",
        "! nohup ollama serve > ollama.log 2>&1 &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpA9vCIboHgF"
      },
      "outputs": [],
      "source": [
        "# download the model\n",
        "! ollama pull llama3.2:3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIMUAsiQl3dl",
        "outputId": "2b5b1f3f-4419-4ca2-f95a-28cd55e975dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME           ID              SIZE      MODIFIED       \n",
            "llama3.2:3b    a80c4f17acd5    2.0 GB    18 seconds ago    \n"
          ]
        }
      ],
      "source": [
        "! ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KPUNULIJi1T"
      },
      "source": [
        "### Action!\n",
        "\n",
        "At this point, we can easily swap the Triage Agent's `llm` with the Llama 3.2 model running on Ollama.\n",
        "\n",
        "We set a `temperature` < 1 to ensure that generated text is more controlled and not too creative.\n",
        "\n",
        "âš ï¸ *Keep in mind that the model is small and that Ollama support for tools is not fully refined yet. As a result, the model may be biased towards generating tool calls (even when not needed) and sometimes may hallucinate tools.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RXrjDXro1Nv"
      },
      "outputs": [],
      "source": [
        "triage_agent.llm = GoogleAIGeminiChatGenerator(api_key=Secret.from_token(\"AIzaSyC5bXfXbB5RRUr3RfKC-tGTALhT-7k0grY\"), model=\"gemini-2.0-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iklIXU3Ko_oe",
        "outputId": "66928b58-85e9-44bf-be08-a9793584fdab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type 'quit' to exit\n",
            "User: hey I need something to catch rats!\n",
            "\n",
            "Sales Agent: Rats? I bet those pesky roadrunners are giving you trouble! Have you tried our Acme Turbo-Charged Roadrunner Trap?\n",
            "User: no I need to get rid of rats\n",
            "\n",
            "Sales Agent: Ah, my mistake! Rats can be quite the nuisance. Have you considered our Acme Supersonic Rat Attractor?\n",
            "User: how it works?\n",
            "\n",
            "Sales Agent: It emits an irresistible high-frequency sound that lures rats into a comfy cage. Completely humane and effortless!\n",
            "User: is it violent?\n",
            "\n",
            "Sales Agent: Not at all! Our Acme Supersonic Rat Attractor is 100% non-violent. The rats simply waltz right in, easy as pie!\n",
            "User: ok great\n",
            "\n",
            "Sales Agent: Fantastic! You'll be rat-free in no time. Now, this cutting-edge technology can be yours for just $9,999!\n",
            "User: litte discount\n",
            "\n",
            "Sales Agent: Well, since you asked nicely, how about a special price of $8,999? It's a steal for such rodent-repelling brilliance!\n",
            "User: yes!!!\n",
            "\n",
            "Sales Agent: Great! Let's get that order processed for you right away!\n",
            "\n",
            "\n",
            "=== Order Summary ===\n",
            "Product: Acme Supersonic Rat Attractor\n",
            "Price: $8999\n",
            "=================\n",
            "\n",
            "Confirm order? y/n: y\n",
            "Order execution successful!\n",
            "\n",
            "Sales Agent: Wonderful! Your order is confirmed. Just one tiny detail - it only works on Tuesdays between 2-4 AM!\n",
            "User: great. now I need help to repair an old ACME product\n",
            "\n",
            "Sales Agent: I see you're an ACME enthusiast! What seems to be malfunctioning with your trusty ACME gadget?\n",
            "User: it is and old ACME SuperKeyboard 5000. does not work on mondays\n",
            "\n",
            "Sales Agent: Ah, the classic ACME SuperKeyboard 5000! Have you tried our new ACME Temporal Typing Tonic?\n",
            "User: i need help with repair!\n",
            "\n",
            "Sales Agent: My apologies, but I'm not equipped to handle repair inquiries. Let me transfer you to someone who can assist better.\n",
            "\n",
            "Triage Agent: Old SuperKeyboard 5000, eh? Don't worry, we've got you covered! Can you please provide me with the error message or symptoms you're experiencing with your keyboard? And just to confirm, it's a Monday, is it not supposed to work today?\n",
            "User: does not work on mondays\n",
            "\n",
            "Issues and Repairs Agent: I see, your ACME SuperKeyboard 5000 isn't working on Mondays. That's quite unusual! Let's dig a bit deeper. Have you noticed any other quirks or issues on other days of the week?\n",
            "User: no\n",
            "\n",
            "Issues and Repairs Agent: I understand. Could you tell me if there are any visible signs of damage or unusual behavior when you try to use it on Mondays?\n",
            "User: vowels are not printed\n",
            "\n",
            "Issues and Repairs Agent: Interesting! So on Mondays, the vowels aren't printing. Have you tried our patented ACME Vowel Vitalizer spray on the keyboard?\n",
            "User: yes\n",
            "\n",
            "Issues and Repairs Agent: I see. Since our standard solution didn't work, let's try something else. Have you checked if the keyboard's \"Monday Mode\" switch is accidentally activated?\n",
            "User: aah. what it is? are you mading it up?\n",
            "\n",
            "Issues and Repairs Agent: I apologize for the confusion. You're right to question that - there's no such thing as a \"Monday Mode\" switch. I shouldn't have suggested a made-up feature. Let's get back to the real issue. Since the ACME Vowel Vitalizer didn't work, would you like me to process a refund for your SuperKeyboard 5000?\n",
            "User: yes pleas\n",
            "\n",
            "Issues and Repairs Agent: Certainly, I'll process that refund for you right away. First, let me look up your item ID.\n",
            "\n",
            "Issues and Repairs Agent: Great, I've found your item ID. Now, I'll execute the refund.\n",
            "\n",
            "\n",
            "=== Refund Summary ===\n",
            "Item ID: item_132612938\n",
            "Reason: Product malfunction - vowels not printing on Mondays\n",
            "=================\n",
            "\n",
            "Refund execution successful!\n",
            "\n",
            "Issues and Repairs Agent: Your refund has been successfully processed.\n",
            "User: quit\n"
          ]
        }
      ],
      "source": [
        "agents = {agent.name: agent for agent in [triage_agent, sales_agent, issues_and_repairs_agent]}\n",
        "\n",
        "print(\"Type 'quit' to exit\")\n",
        "\n",
        "messages = []\n",
        "current_agent_name = \"Triage Agent\"\n",
        "\n",
        "while True:\n",
        "    agent = agents[current_agent_name]\n",
        "\n",
        "    if not messages or messages[-1].role == ChatRole.ASSISTANT:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "        messages.append(ChatMessage.from_user(user_input))\n",
        "\n",
        "    current_agent_name, new_messages = agent.run(messages)\n",
        "    messages.extend(new_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZl11DGxCR2u"
      },
      "source": [
        "In conclusion, we have built a multi-agent system using Swarm concepts and Haystack tools, demonstrating how to integrate models from different providers, including a local model running on Ollama.\n",
        "\n",
        "Swarm ideas are pretty simple and useful for several use cases and the abstractions provided by Haystack make it easy to implement them.\n",
        "However, this architecture may not be the best fit for all use cases: memory is handled as a list of messages; this system only runs one Agent at a time.\n",
        "\n",
        "Looking ahead, we plan to develop and showcase more advanced Agents with Haystack. Stay tuned! ðŸ“»"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2UHIbcVCR2u"
      },
      "source": [
        "## Notebooks on Tool support\n",
        "- [ðŸ› ï¸ Define & Run Tools](https://haystack.deepset.ai/cookbook/tools_support)\n",
        "- [ðŸ“° Newsletter Sending Agent](https://haystack.deepset.ai/cookbook/newsletter-agent)\n",
        "\n",
        "(Notebook by [Stefano Fiorucci](https://github.com/anakin87))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}